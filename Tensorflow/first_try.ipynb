{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49000, 32, 32, 3)\n",
      "Train data shape:  (49000, 32, 32, 3)\n",
      "Train labels shape:  (49000,) int32\n",
      "Validation data shape:  (1000, 32, 32, 3)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (10000, 32, 32, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import load_cifar10 \n",
    "from Trainer import *\n",
    "\n",
    "def load_cifar10(num_training=49000, num_validation=1000, num_test=10000):\n",
    "\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier.\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "    #print(mean_pixel.shape)\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        return iter((self.X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import tensorflow as tf\n",
    "\n",
    "def train_part34(model_init_fn, optimizer_init_fn, num_epochs=1, is_training=False):\n",
    "\n",
    "    with tf.device(device):\n",
    "\n",
    "        # Compute the loss like we did in Part II\n",
    "        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "        \n",
    "        model = model_init_fn()\n",
    "        optimizer = optimizer_init_fn()\n",
    "        \n",
    "        train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "        train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "    \n",
    "        val_loss = tf.keras.metrics.Mean(name='val_loss')\n",
    "        val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n",
    "        \n",
    "        t = 0\n",
    "        for epoch in range(num_epochs):\n",
    "            \n",
    "            # Reset the metrics - https://www.tensorflow.org/alpha/guide/migration_guide#new-style_metrics\n",
    "            train_loss.reset_states()\n",
    "            train_accuracy.reset_states()\n",
    "            \n",
    "            for x_np, y_np in train_dset:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    \n",
    "                    # Use the model function to build the forward pass.\n",
    "                    scores = model(x_np, training=is_training)\n",
    "                    loss = loss_fn(y_np, scores)\n",
    "      \n",
    "                    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "                    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                    \n",
    "                    # Update the metrics\n",
    "                    train_loss.update_state(loss)\n",
    "                    train_accuracy.update_state(y_np, scores)\n",
    "                    \n",
    "                    if t % print_every == 0:\n",
    "                        val_loss.reset_states()\n",
    "                        val_accuracy.reset_states()\n",
    "                        for test_x, test_y in val_dset:\n",
    "                            # During validation at end of epoch, training set to False\n",
    "                            prediction = model(test_x, training=False)\n",
    "                            t_loss = loss_fn(test_y, prediction)\n",
    "\n",
    "                            val_loss.update_state(t_loss)\n",
    "                            val_accuracy.update_state(test_y, prediction)\n",
    "                        \n",
    "                        template = 'Iteration {}, Epoch {}, Loss: {}, Accuracy: {}, Val Loss: {}, Val Accuracy: {}'\n",
    "                        print (template.format(t, epoch+1,\n",
    "                                             train_loss.result(),\n",
    "                                             train_accuracy.result()*100,\n",
    "                                             val_loss.result(),\n",
    "                                             val_accuracy.result()*100))\n",
    "                    t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomConvNet(tf.keras.Model):\n",
    "    def __init__(self, channel_1, channel_2,channel_3, num_classes):\n",
    "        super(CustomConvNet, self).__init__()\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        #initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "        initializer = tf.keras.initializers.GlorotNormal()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filters = channel_1, kernel_size = [3, 3],\n",
    "                                           strides = 1, padding='same', kernel_initializer=initializer)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.drop1 = tf.keras.layers.Dropout(rate = 0.5)\n",
    "        self.max_pool1 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = [2, 2])\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filters = channel_2, kernel_size = [3, 3],\n",
    "                                           strides = 1, padding='valid', kernel_initializer=initializer)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.drop2 = tf.keras.layers.Dropout(rate = 0.5)\n",
    "        self.max_pool2 = tf.keras.layers.MaxPool2D(pool_size = [2, 2], strides = [2, 2])\n",
    "        \n",
    "        self.conv3 = tf.keras.layers.Conv2D(filters = channel_3, kernel_size = [3, 3],\n",
    "                                           strides = 1, padding='valid', kernel_initializer=initializer)\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu3 = tf.keras.layers.ReLU()\n",
    "        self.drop3 = tf.keras.layers.Dropout(rate = 0.5)\n",
    "        self.avg_pool3 = tf.keras.layers.AveragePooling2D(pool_size = [5, 5], strides = [1, 1])\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc4 = tf.keras.layers.Dense(100, kernel_initializer=initializer)\n",
    "        self.fc5 = tf.keras.layers.Dense(50, kernel_initializer=initializer)\n",
    "        self.fc6 = tf.keras.layers.Dense(num_classes, activation='softmax', kernel_initializer=initializer)\n",
    "        \n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "    \n",
    "    def call(self, input_tensor, training=False):\n",
    "        ############################################################################\n",
    "        # TODO: Construct a model that performs well on CIFAR-10                   #\n",
    "        ############################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        x = input_tensor\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.max_pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.avg_pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        x = self.fc6(x)\n",
    "        \n",
    "        #channel_1, channel_2, num_classes = 16, 8, 10\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        ############################################################################\n",
    "        #                            END OF YOUR CODE                              #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1585206161.7679274\n",
      "Iteration 0, Epoch 1, Loss: 2.459230899810791, Accuracy: 7.8125, Val Loss: 2.3138442039489746, Val Accuracy: 7.800000190734863\n",
      "Iteration 700, Epoch 1, Loss: 1.4269778728485107, Accuracy: 48.161109924316406, Val Loss: 2.801622152328491, Val Accuracy: 20.0\n",
      "Iteration 1400, Epoch 2, Loss: 1.1612461805343628, Accuracy: 58.656494140625, Val Loss: 3.5124359130859375, Val Accuracy: 18.899999618530273\n",
      "Iteration 2100, Epoch 3, Loss: 1.0613291263580322, Accuracy: 62.17596435546875, Val Loss: 3.523514747619629, Val Accuracy: 17.80000114440918\n",
      "Iteration 2800, Epoch 4, Loss: 0.9850450158119202, Accuracy: 64.916748046875, Val Loss: 3.7371129989624023, Val Accuracy: 19.100000381469727\n",
      "Iteration 3500, Epoch 5, Loss: 0.9392223358154297, Accuracy: 66.82994842529297, Val Loss: 3.6673688888549805, Val Accuracy: 17.80000114440918\n",
      "Iteration 4200, Epoch 6, Loss: 0.9028566479682922, Accuracy: 68.02560424804688, Val Loss: 3.4935266971588135, Val Accuracy: 21.899999618530273\n",
      "Iteration 4900, Epoch 7, Loss: 0.8607014417648315, Accuracy: 69.67725372314453, Val Loss: 2.6958227157592773, Val Accuracy: 28.400001525878906\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "print(tic)\n",
    "train_dset = Dataset(X_train, y_train, batch_size=64, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=64, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=64)\n",
    "    \n",
    "    \n",
    "#device = '/device:GPU:0'   # Change this to a CPU/GPU as you wish!\n",
    "device = '/cpu:0'        # Change this to a CPU/GPU as you wish!\n",
    "print_every = 700\n",
    "num_epochs = 10\n",
    "channel_1, channel_2, channel_3, num_classes = 32, 64, 128, 10\n",
    "model = CustomConvNet(channel_1, channel_2, channel_3, num_classes)\n",
    "\n",
    "def model_init_fn():\n",
    "    return CustomConvNet(channel_1, channel_2, channel_3, num_classes)\n",
    "\n",
    "def optimizer_init_fn():\n",
    "    learning_rate = 1e-3\n",
    "    return tf.keras.optimizers.Adam(learning_rate) \n",
    "train_part34(model_init_fn, optimizer_init_fn, num_epochs=num_epochs, is_training=True)\n",
    "\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
